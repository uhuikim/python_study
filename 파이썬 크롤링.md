# python 데이터 크롤링 하기

 수업시간에 배운 크롤링 방법 두가지(어떤 라이브러리가 어떤 함수를 가지고 있는지 암기하기)

1. BeatifulSoup & requests
2. Selenium & chromedriver 

------

#### < BeautifulSoup>

-  정의 : HTML 및 XML파일에서 원하는 데이터를 손쉽게 Parsing 할 수 있는 Python 라이브러리
  
-  HTML 파싱은 텍스트 형태의 HTML 코드를 분석해서 객체로 만든 뒤 검색하거나 편집할 수 있도록 만드는 작업
  
-  html등의 코드를 python이 이해하는 객체구조로 변환하는 parsing을 맡고 있음
  
  - parsing이란? 일련의 문자열을 의미있는 token(어휘 분석의 단위)으로 분해하고 그것들로 이루어진 parse tree를 만드는 과정
  
  - requests 모듈 :
  
    python에서 http요청을 보내는 모듈(http통신을 위한 파이썬 라이브러리)
  
    requests는 html을  python이 이해하는 객체구조로 만들어 주지는 못해 beatifulsoup을 이용한다. 



##### url에서 html 소스코드 가져오기

```python
from bs4 import BeatifulSoup
import requests 

#크롤링 할 url저장
url = "www.naver.com"
#url페이지에 접속하고 페이지의 내용을 response에 대입 
response = requests.get(url)
#응답정보 한글 설정
response.encoding = "UTF8"
response.text

#응답정보를 저장하고 응답정보에서 정보를 추출하는 beatifulsoup 객체 생성
# 첫 인자는 html 소스코드, 두번째 인자는 어떤 parser를 이용할지 명시
soup = BeatifulSoup(response.text , 'html.parser')
```

##### 태그를 이용한 정보 추출 방법 

```python
#body태그안에 포함된 내용 리턴
body = soup.body
#body 태그안에 포함된 h1 태그를 리턴
h1=body.h1
#h1태그에 포함된 문자열 리턴 
h1.string
```

##### id속성을 이용한 정보추출(id값은 유일한 값)

```python
soup = BeautifulSoup(response.text , 'html.parser')
#<html>
#<body>
#<h1 id='title'>스크레핑이란?</h1>
#<p id='body01'>웹 페이지를 분석하는 것</p>
#<p id='body02'>원 하는 데이터를 추출 하는것</p>
#</body>
#</html>

#id속성이 title인 객체 리턴 
title = soup.find(id='title')
#---> <h1 id='title'>스크레핑이란?</h1>

#id속성이 body01인 객체 리턴
body01 = soup.find(id="body01")
#---><p id='body01'>웹 페이지를 분석하는 것</p>
```

##### find 함수를 이용한 정보 추출 

- find() : 해당 조건에 맞는 하나의 태그를 가져온다. 중복이면 가장 첫 번째 태그를 가져온다.
- find(name, attrs, recursive, string, **kwargs)
- 태그와 속성 이용
  - find('태그명', {'속성명' : '속성값' ...})

```python

```

#####  find_all함수를 이용한 정보 추출

- find_all() : 해당 조건에 맞는 모든 태그들을 가져온다.
- find_all(name, attrs, recursive, string, limit, **kwargs)
- 태그와 속성 이용
  - find_all('태그명', {'속성명' : '속성값' ...})

```python
#find_all() 메서드로 a태그 추출하기
links = soup.find_all("a")
```

------

##### find 와 find_all 

속성명이 한개인지 여러개인지 알아야 find를 쓸지 find_all을 쓸지 정할 수 있음 

ctrl+f12번으로 검색해서 몇개인지 확인한다.

![검색 화면](C:\코딩 공부 정리\파이썬 관련\find& find_all.PNG)

------

#####  select_one & select 를 이용한 정보 추출 

f12버튼 / copy / copy selector 이용

```python
import requests
from bs4 import BeautifulSoup

url = 'https://gist.githubusercontent.com/eduChange-hphk/3f1770767ef61105b608244f0d1433f7/raw/23ad99a4786d88b76667f5b7d312cc0d36318c7b/selector.html'

response = requests.get(url)

html = response.text
soup = BeautifulSoup(html, 'html.parser')
select = soup.select_one('body > ol.subway > li:nth-child(3)')

print(select.text)

#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ

import requests
from bs4 import BeautifulSoup

url = "http://www.naver.com"
response = requests.get(url)
html = response.text
soup = BeautifulSoup(html,'html.parser')
select = soup.select('#PM_ID_ct > div.header > div.section_navbar > div.area_hotkeyword.PM_CL_realtimeKeyword_base > div.ah_roll.PM_CL_realtimeKeyword_rolling_base > div > ul >li >a >span.ah_k')
qq = []
for i in select :
    qq.append(i.text)
    
print(qq[0:10])
```

------

##### 날씨 크롤링 실습해보기

``` python
from bs4 import BeautifulSoup
import requests

url = "http://www.kma.go.kr/weather/observation/currentweather.jsp"

response = requests.get(url).text
soup = BeautifulSoup(response,'html.parser')
table = soup.find('table',{'class' : "table_develop3"})
data = []   #데이터를 저장할 리스트 생성
for tr in table.find_all('tr'):
    tds = list(tr.find_all('td'))
    
    for td in tds:
        if td.find('a'):
            point = td.find('a').text
            temperature = tds[5].text
            humidity = tds[10].text
            data.append([point,temperature,humidity])            
print(data)
```

------

#### < Selenium>







------

##### 크롤링 데이터 os이용해서 csv파일로 저장하기 

```python
import os #폴더를 만들어주는 라이브러리(뻐꾸기라는 폴더에 파일 저장)

os.path.exists("c:/ai/python/뻐꾸기") #폴더가 존재하는지 확인
os.mkdir("c:/ai/python/뻐꾸기") #폴더를 만들어 준다. 

#위 두개 합친 버전
if os.path.exists("c:/ai/workspace/python/뻐꾸기")==False:
    os.mkdir("c:/ai/workspace/python/뻐꾸기")
    
#파일 저장하기
with open('c:/ai/python/뻐꾸기/wether.csv','w') as file: 
    #'w'저장하려고 열겠다.weather.csv에 저장할 객체 file 리턴 / file이 저장함  
    file.write('point, temperature, humidity\n') #저장할 내용
    for i in data:
        file.write('{0},{1},{2}\n'.format(i[0],i[1],i[2]))
        #format 이랑 같은 표현식({}에 넣을 값을 나중에 알려줄게)
        #f-string
        #file.write(f"{i[0],i[1]}")
```

weather.csv 파일을 open

1. weather.csv 존재하지 않음 -> 빈파일 weather.csv 생성 , 파일열기 
2. weather.csv 존재함 -> weather.csv내용 삭제 , 파일열기

데이터를 출력하고 싶을때 with open 

w는 저장하려고 연다. 



##### 크롤링 데이터 판다스 이용해서 csv파일로 저장하기 

```python
import pandas as pd   #판다스 임포트 
df= pd.DataFrame(data) # 데이터 프레임 만들기 
df.to_csv("ohohohoh.csv",encoding ="euc-kr")  #csv로 저장 , 엑셀 인코딩은 euc-kr
```

